{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/malekkhammassi/A-CNN-based-Stereo-Image-Denoising-Method/blob/master/Stereo_Image_Denoising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcE3GRKtZJJQ"
   },
   "source": [
    "## **The implementation is based on the framework Pytorch and the network was trained and tested using Google Colab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uX9D2ulZZgie"
   },
   "source": [
    "### 1.   Mounting the Drive\n",
    "This process is necessary to connect google drive to google colab and therefore to load the data into google colab.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "orKeXskDwrX1",
    "outputId": "ae28e113-425e-43e1-dbb2-5b135d5711a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YdY56jV9ibk"
   },
   "source": [
    "### 2.   Preparing the Environment\n",
    "In order to prepare the suitable environment for the code, we provide a requirements.txt file on our github project.\n",
    "\n",
    "\n",
    "1.   Download the requirements file or copy/paste its content on your laptop.\n",
    "2.   Upload the file by executing the next cell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Aq4kqifQM_H2",
    "outputId": "139e7e3f-41f0-4088-8887-4f44e37754cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-f2a1a0f4-33ac-4f84-b918-234679795464\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-f2a1a0f4-33ac-4f84-b918-234679795464\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving requirements.txt to requirements.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'requirements.txt': b'opencv-python==3.4.1.15\\nscikit-image==0.12.3\\ntorch==0.4.0\\ntorchvision==0.2.1\\nh5py==2.7.0\\n\\n\\nnumpy==1.14.4'}"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload the requirements file\n",
    "from google.colab import files \n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-00kCjCSfdhX"
   },
   "source": [
    "The next step is to install the requirements by executing the next four cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBcSFWadNHVb"
   },
   "outputs": [],
   "source": [
    "#install the requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8-f2uL2OhpB"
   },
   "outputs": [],
   "source": [
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EX8w91babcEw"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-image --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1y2IaixBO1ox"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/lanpa/tensorboardX && cd tensorboardX && python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HHXJ_WvdaTjo"
   },
   "source": [
    "### 3. Installing the Necessary Libreries \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGtN7Ea4aikE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import h5py\n",
    "import torch\n",
    "import glob\n",
    "import torch.utils.data as udata\n",
    "import re\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from statistics import mean\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage.measure import compare_psnr, compare_ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hbswiQa-9fp"
   },
   "source": [
    "The next cell contains necessary functions needed during training and testing:\n",
    "1. **weights_init_kaiming** initializes the weights of the networks in the first epoch of training.\n",
    "2. **batch_PSNR** Computes the PSNR of a batch of 128 images.\n",
    "3. **batch_SSIM** Computes the SSIM of a batch of 128 images.\n",
    "4. **findLastCheckpoint** finds the last epoch of training.\n",
    "5. **data_augmentation** applies data augmentation on one image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8V0azKmsPFSk"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        # nn.init.uniform(m.weight.data, 1.0, 0.02)\n",
    "        m.weight.data.normal_(mean=0, std=math.sqrt(2./9./64.)).clamp_(-0.025,0.025)\n",
    "        nn.init.constant(m.bias.data, 0.0)\n",
    "\n",
    "def batch_PSNR(img, imclean, data_range):\n",
    "    Img = img.data.cpu().numpy().astype(np.float32)\n",
    "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
    "    PSNR = 0\n",
    "    for i in range(Img.shape[0]):\n",
    "        im1 = Iclean[i,:,:,:].transpose((1,2,0))\n",
    "        im2 =  Img[i,:,:,:].transpose((1,2,0))\n",
    "        PSNR += compare_psnr(im1, im2)\n",
    "    return (PSNR/Img.shape[0])\n",
    "\n",
    "def batch_SSIM(img, imclean):\n",
    "    Img = img.data.cpu().numpy().astype(np.float32)\n",
    "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
    "    SSIM = 0\n",
    "    for i in range(Img.shape[0]):\n",
    "        im1 = Iclean[i,:,:,:].transpose((1,2,0))\n",
    "        im2 =  Img[i,:,:,:].transpose((1,2,0))\n",
    "        SSIM += compare_ssim(im1, im2, multichannel=True)\n",
    "    return ( SSIM/Img.shape[0])\n",
    "  \n",
    "def findLastCheckpoint(save_dir):\n",
    "    file_list = glob.glob(os.path.join(save_dir, 'model_*.pth'))\n",
    "    if file_list:\n",
    "        epochs_exist = []\n",
    "        for file_ in file_list:\n",
    "            result = re.findall(\".*model_(.*).pth.*\", file_)\n",
    "            epochs_exist.append(int(result[0]))\n",
    "        initial_epoch = max(epochs_exist)\n",
    "    else:\n",
    "        initial_epoch = 0\n",
    "    return initial_epoch\n",
    "\n",
    "def data_augmentation(image, mode):\n",
    "    out = np.transpose(image, (1,2,0))\n",
    "    if mode == 0:\n",
    "        # original\n",
    "        out = out\n",
    "    elif mode == 1:\n",
    "        # flip up and down\n",
    "        out = np.flipud(out)\n",
    "    elif mode == 2:\n",
    "        # rotate counterwise 90 degree\n",
    "        out = np.rot90(out)\n",
    "    elif mode == 3:\n",
    "        # rotate 90 degree and flip up and down\n",
    "        out = np.rot90(out)\n",
    "        out = np.flipud(out)\n",
    "    elif mode == 4:\n",
    "        # rotate 180 degree\n",
    "        out = np.rot90(out, k=2)\n",
    "    elif mode == 5:\n",
    "        # rotate 180 degree and flip\n",
    "        out = np.rot90(out, k=2)\n",
    "        out = np.flipud(out)\n",
    "    elif mode == 6:\n",
    "        # rotate 270 degree\n",
    "        out = np.rot90(out, k=3)\n",
    "    elif mode == 7:\n",
    "        # rotate 270 degree and flip\n",
    "        out = np.rot90(out, k=3)\n",
    "        out = np.flipud(out)\n",
    "    return np.transpose(out, (2,0,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UdEnOQR_HAM"
   },
   "source": [
    "# **Model**\n",
    "The next cell contains the architecture of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_dXlOBWPnkn"
   },
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, channels, num_of_layers=17):\n",
    "        super(DnCNN, self).__init__()\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        features = 64\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "    \n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(num_of_layers-2):\n",
    "            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(features))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = self.dncnn(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1AGhH2vQ_RXP"
   },
   "source": [
    "# **Dataset**\n",
    "The next cell prepares the data for the training:\n",
    "1. **normalize** applies normalization on the images.\n",
    "2. **Im2Patch** prepares the patches.\n",
    "3. **prepare_data** contructs a 3D image out of the two views (left and right), it calls Im2Patch, data_augmentation and normalize to prepare the training patches and it also prepares the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZFRSJlC5q3D"
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return data/255.\n",
    "\n",
    "def Im2Patch(img, win, stride=1):\n",
    "    k = 0\n",
    "    endc = img.shape[2]\n",
    "    endw = img.shape[1]\n",
    "    endh = img.shape[0]\n",
    "    patch = img[0:endw-win+0+1:stride, 0:endh-win+0+1:stride,:]\n",
    "    TotalPatNum = patch.shape[1] * patch.shape[0]\n",
    "    Y = np.zeros([endc, win*win,TotalPatNum], np.float32)\n",
    "    for i in range(win):\n",
    "        for j in range(win):\n",
    "            patch = img[i:endw-win+i+1:stride,j:endh-win+j+1:stride,:]\n",
    "            Y[:,k,:] = np.array(np.resize(patch[:],(endc, TotalPatNum)))\n",
    "            k = k + 1\n",
    "    return Y.reshape([endc, win, win, TotalPatNum])\n",
    "\n",
    "def prepare_data(data_path, patch_size, stride, aug_times=1):\n",
    "    # training data\n",
    "    print('process training data')\n",
    "    scales = [1]\n",
    "    files_right = glob.glob(os.path.join(data_path, 'train_right', '*.png'))\n",
    "    files_left = glob.glob(os.path.join(data_path, 'train_left', '*.png'))\n",
    "    files_right.sort()\n",
    "    files_left.sort()\n",
    "    h5f = h5py.File(os.path.join(data_path, 'train.h5'), 'w')\n",
    "    train_num = 0\n",
    "    for i in range(len(files_right)):\n",
    "        img_right = cv2.imread(files_right[i],0)\n",
    "        img_left = cv2.imread(files_left[i],0)\n",
    "        img_right= np.expand_dims(img_right,2)\n",
    "        img_left = np.expand_dims(img_left,2)\n",
    "        hr, wr, cr = img_right.shape\n",
    "        img=np.zeros((hr,wr,2))\n",
    "        img[:,:,0]=img_right[:,:,0]\n",
    "        img[:,:,1]=img_left[:,:,0]\n",
    "        h, w, c = img.shape #img is a 3D image that has the right view on the first channel and the left view on the second channel\n",
    "        for k in range(len(scales)):\n",
    "            Img = cv2.resize(img, (int(h*scales[k]), int(w*scales[k])), interpolation=cv2.INTER_CUBIC)\n",
    "            Img = np.float32(normalize(Img))\n",
    "            patches = Im2Patch(Img, win=patch_size, stride=stride)\n",
    "            print(\"file: %s scale %.1f # samples: %d\" % (files_right[i], scales[k], patches.shape[3]*aug_times))\n",
    "            for n in range(patches.shape[3]):\n",
    "                data = patches[:,:,:,n].copy()\n",
    "                h5f.create_dataset(str(train_num), data=data)\n",
    "                train_num += 1\n",
    "                for m in range(aug_times-1):\n",
    "                    data_aug = data_augmentation(data, np.random.randint(1,8))\n",
    "                    h5f.create_dataset(str(train_num)+\"_aug_%d\" % (m+1), data=data_aug)\n",
    "                    train_num += 1\n",
    "    h5f.close()\n",
    "    # validation data\n",
    "    print('\\nprocess validation data')\n",
    "    files_right.clear()\n",
    "    files_left.clear()\n",
    "    files_right = glob.glob(os.path.join(data_path, 'validation_right', '*.png'))\n",
    "    files_left  = glob.glob(os.path.join(data_path, 'validation_left' , '*.png'))\n",
    "    files_right.sort()\n",
    "    files_left.sort()\n",
    "    h5f = h5py.File(os.path.join(data_path, 'val.h5'), 'w')\n",
    "    val_num = 0\n",
    "    for i in range(len(files_right)):\n",
    "        print(\"files: \", files_right[i], files_left[i])\n",
    "        img_right = cv2.imread(files_right[i],0)\n",
    "        img_left  = cv2.imread(files_left [i],0)\n",
    "        img_right= np.expand_dims(img_right,2)\n",
    "        img_left = np.expand_dims(img_left,2)\n",
    "        hv, wv, cv= img_right.shape\n",
    "        img=np.zeros((2,hv,wv))\n",
    "        img[0,:,:]=img_right[:,:,0]\n",
    "        img[1,:,:]=img_left[:,:,0]\n",
    "        img = np.float32(normalize(img)) #img is a 3D image that has the right view on the first channel and the left view on the second channel\n",
    "        h5f.create_dataset(str(val_num), data=img)\n",
    "        val_num += 1\n",
    "    h5f.close()\n",
    "    print('training set, # samples %d\\n' % train_num)\n",
    "    print('val set, # samples %d\\n' % val_num)\n",
    "\n",
    "class Dataset(udata.Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            h5f = h5py.File(os.path.join(data_path, 'train.h5') ,'r')\n",
    "        else:\n",
    "            h5f = h5py.File(os.path.join(data_path, 'val.h5'), 'r')\n",
    "        self.keys = list(h5f.keys())\n",
    "        random.shuffle(self.keys)\n",
    "        h5f.close()\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            h5f = h5py.File(os.path.join(data_path, 'train.h5') , 'r')\n",
    "        else:\n",
    "            h5f = h5py.File(os.path.join(data_path, 'val.h5') , 'r')\n",
    "        key = self.keys[index]\n",
    "        data = np.array(h5f[key])\n",
    "        h5f.close()\n",
    "        return torch.Tensor(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1OCoNE47AkN"
   },
   "source": [
    "# **Training**\n",
    "The hyperparamters are defined at the beginning of the code:\n",
    "\n",
    "**preprocess** = False if the data is already preprocessed.\n",
    "\n",
    "**batchSize**     = 128   is the batch size.\n",
    "\n",
    "**num_of_layers** = 17    is the number of layers.\n",
    "\n",
    "**epochs**        = 50    is the number of the training epochs.\n",
    "\n",
    "**milestone**     = 30    is the number of milestone (the number of epochs where we change the learning rate).\n",
    "\n",
    "**lr** = 1e-3  is the learning rate.\n",
    "\n",
    "**outf** = \"/content/drive/My Drive/Stereo-PyTorch-master/logs/new_training2\" is the file that will contain the trained models. \n",
    "\n",
    "**mode** is \"S\" is the mode is \"S\" if the training is for a specific noise level and it is \"B\" if the training is for blind denoising.\n",
    "\n",
    "**noiseL** = 35  is the noise level during training.\n",
    "\n",
    "**val_noiseL** = 35 is the noise level during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQRIYYXo8z78"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "preprocess    = True  #preprocess = False if the data is already preprocessed\n",
    "batchSize     = 128   #the batch size\n",
    "num_of_layers = 17    #the number of layers\n",
    "epochs        = 50    #the number of the training epochs\n",
    "milestone     = 30    #the number of milestone (the number of epochs where we change the learning rate)\n",
    "lr            = 1e-3  #the learning rate\n",
    "outf          = \"/content/drive/My Drive/Stereo-PyTorch-master/logs/new_training\" # the file of containig the trained models \n",
    "mode          = \"S\" # the mode is \"S\" if the training is for a specific noise level and it is \"B\" if the training is for blind denoising.\n",
    "noiseL        = 35  # the noise level during training.\n",
    "val_noiseL    = 35  # the noise level during validation.\n",
    "data_path     = '/content/drive/My Drive/Stereo-PyTorch-master/data'\n",
    "\n",
    "def main():\n",
    "    # Load dataset\n",
    "    print('Loading dataset ...\\n')\n",
    "    dataset_train = Dataset(train=True)\n",
    "    dataset_val = Dataset(train=False)\n",
    "    loader_train = DataLoader(dataset=dataset_train, num_workers=4, batch_size=batchSize, shuffle=True)\n",
    "    print(\"# of training samples: %d\\n\" % int(len(dataset_train)))\n",
    "    \n",
    "    # Build model and find last check point\n",
    "    net = DnCNN(channels=2, num_of_layers=num_of_layers)\n",
    "    initial_epoch = findLastCheckpoint(save_dir=outf)\n",
    "\n",
    "    # Loading the last trained model and the previous values of PSNR and Loss\n",
    "    if initial_epoch > 0:\n",
    "      print('resuming by loading epoch %03d' % initial_epoch)\n",
    "      net = torch.load(os.path.join(outf , 'model_%03d.pth' % initial_epoch))\n",
    "      loss_epoch_train = np.load(os.path.join(outf,'loss_epoch_train.npy'))\n",
    "      loss_epoch_val = np.load(os.path.join(outf,'loss_epoch_val.npy'))\n",
    "      PSNR_epoch_train = np.load(os.path.join(outf,'PSNR_epoch_train.npy'))\n",
    "      PSNR_epoch_val = np.load(os.path.join(outf,'PSNR_epoch_val.npy'))\n",
    "    else:\n",
    "      print('we are starting from the first epoch')\n",
    "      net.apply(weights_init_kaiming)\n",
    "      loss_epoch_train = []\n",
    "      loss_epoch_val = []\n",
    "      PSNR_epoch_train = []\n",
    "      PSNR_epoch_val = []\n",
    "      \n",
    "    criterion = nn.MSELoss(size_average=False)\n",
    "    \n",
    "    # Move to GPU\n",
    "    device_ids = [0]\n",
    "    model = nn.DataParallel(net, device_ids=device_ids).cuda()\n",
    "    criterion.cuda()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # training\n",
    "    writer = SummaryWriter(outf)\n",
    "    step = 0\n",
    "    noiseL_B=[0,55] # ingnored when opt.mode=='S'\n",
    "    for epoch in range(initial_epoch, epochs):\n",
    "        if epoch < milestone:\n",
    "            current_lr = lr\n",
    "        else:\n",
    "            current_lr = lr / 10.\n",
    "        # set learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = current_lr\n",
    "        print('learning rate %f' % current_lr)\n",
    "        \n",
    "        \n",
    "        # train\n",
    "        loss_train=[]\n",
    "        PSNR_train=[]\n",
    "        for i, data in enumerate(loader_train, 0):\n",
    "            # training step\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            img_train = data\n",
    "            if mode == 'S':\n",
    "                noise = torch.FloatTensor(img_train.size()).normal_(mean=0, std=noiseL/255.)\n",
    "                \n",
    "            if mode == 'B':\n",
    "                noise = torch.zeros(img_train.size())\n",
    "                stdN = np.random.uniform(noiseL_B[0], noiseL_B[1], size=noise.size()[0])\n",
    "                for n in range(noise.size()[0]):\n",
    "                    sizeN = noise[0,:,:,:].size()\n",
    "                    noise[n,:,:,:] = torch.FloatTensor(sizeN).normal_(mean=0, std=stdN[n]/255.)\n",
    "            imgn_train = img_train + noise\n",
    "            img_train, imgn_train = Variable(img_train.cuda()), Variable(imgn_train.cuda())\n",
    "            noise = Variable(noise.cuda())\n",
    "            out_train = model(imgn_train)\n",
    "            loss = criterion(out_train, noise) / (imgn_train.size()[0]*2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # results\n",
    "            model.eval()\n",
    "            out_train = torch.clamp(imgn_train-model(imgn_train), 0., 1.)\n",
    "            psnr_train = batch_PSNR(out_train, img_train, 1.)\n",
    "            loss_train.append(loss.item())\n",
    "            print(loss.item())\n",
    "            PSNR_train.append(psnr_train)\n",
    "            print(\"[epoch %d][%d/%d] loss: %.4f PSNR_train: %.4f\" %\n",
    "                (epoch+1, i+1, len(loader_train), loss.item(), psnr_train))\n",
    "            # if you are using older version of PyTorch, you may need to change loss.item() to loss.data[0]\n",
    "            if step % 10 == 0:\n",
    "                # Log the scalar values\n",
    "                writer.add_scalar('loss', loss.item(), step)\n",
    "                writer.add_scalar('PSNR on training data', psnr_train, step)\n",
    "            step += 1\n",
    "        ## the end of each epoch\n",
    "        model.eval()\n",
    "        if((type(loss_epoch_train) == list) and (epoch == 0)):\n",
    "          loss_epoch_train.append(mean(loss_train))\n",
    "          PSNR_epoch_train.append(mean(PSNR_train))\n",
    "          loss_epoch_train = np.asarray(loss_epoch_train)\n",
    "          PSNR_epoch_train = np.asarray(PSNR_epoch_train)\n",
    "        else:\n",
    "          loss_epoch_train = np.append(loss_epoch_train,mean(loss_train))\n",
    "          PSNR_epoch_train = np.append(PSNR_epoch_train,mean(PSNR_train))\n",
    "        \n",
    "        np.save(os.path.join(outf,'loss_epoch_train.npy'),loss_epoch_train)\n",
    "        np.save(os.path.join(outf,'PSNR_epoch_train.npy'),PSNR_epoch_train)\n",
    "        print(\"[on the whole epoch %d][%d/%d] loss: %.4f PSNR_train: %.4f\" %\n",
    "                (epoch+1, i+1, len(loader_train), mean(loss_train),mean(PSNR_train)))\n",
    "        \n",
    "        # validate\n",
    "        psnr_val = 0\n",
    "        loss_val = 0\n",
    "        for k in range(len(dataset_val)):\n",
    "          img_val = dataset_val[k]\n",
    "          img_val = torch.unsqueeze(img_val,0)\n",
    "          noise = torch.FloatTensor(img_val.size()).normal_(mean=0, std=val_noiseL/255.)\n",
    "          imgn_val = img_val + noise\n",
    "          img_val, imgn_val = Variable(img_val.cuda(), volatile=True), Variable(imgn_val.cuda(), volatile=True)\n",
    "          noise = Variable(noise.cuda())\n",
    "          out_val=model(imgn_val)\n",
    "          loss = criterion(out_val, noise) / (imgn_val.size()[0]*2)\n",
    "          out_val = torch.clamp(imgn_val-model(imgn_val), 0., 1.)\n",
    "          psnr_val += batch_PSNR(out_val, img_val, 1.)\n",
    "          loss_val += loss.item()\n",
    "          print(loss.item())\n",
    "        psnr_val /= len(dataset_val)\n",
    "        loss_val /= len(dataset_val)\n",
    "        if((type(loss_epoch_val) == list) and (epoch == 0)):\n",
    "          loss_epoch_val.append(loss_val)\n",
    "          PSNR_epoch_val.append(psnr_val)\n",
    "          loss_epoch_val = np.asarray(loss_epoch_val)\n",
    "          PSNR_epoch_val = np.asarray(PSNR_epoch_val)\n",
    "        else:\n",
    "          loss_epoch_val = np.append(loss_epoch_val,loss_val)\n",
    "          PSNR_epoch_val = np.append(PSNR_epoch_val,psnr_val)\n",
    "        np.save(os.path.join(outf,'loss_epoch_val.npy'),loss_epoch_val)\n",
    "        np.save(os.path.join(outf,'PSNR_epoch_val.npy'),PSNR_epoch_val)\n",
    "        print(\"\\n[epoch %d] PSNR_val: %.4f\" % (epoch+1, psnr_val))\n",
    "        print(\"\\n[epoch %d] loss_val: %.4f\" % (epoch+1, loss_val))\n",
    "        writer.add_scalar('PSNR on validation data', psnr_val, epoch)\n",
    "        \n",
    "    \n",
    "        out_train = torch.clamp(imgn_train-model(imgn_train), 0., 1.)\n",
    "        # save model\n",
    "        torch.save(model, os.path.join(outf , 'model_%03d.pth' % (epoch+1)))\n",
    "        \n",
    "\t#plot graphs\n",
    " \n",
    "\n",
    "    x=[i for i in range(1,epochs+1)]\n",
    "    loss_epoch_train = list(loss_epoch_train)\n",
    "    loss_epoch_val = list(loss_epoch_val)\n",
    "    PSNR_epoch_train = list(PSNR_epoch_train)\n",
    "    PSNR_epoch_val = list(PSNR_epoch_val)\n",
    "    #plt.subplot(211)\n",
    "    fig1=plt.figure()\n",
    "    plt.title(\"loss function on both training set and validation set\")\n",
    "    plt.plot(x,loss_epoch_train,'-r', label='Training')\n",
    "    print(len(x))\n",
    "    print(len(loss_epoch_train))\n",
    "    plt.plot(x,loss_epoch_val,'-b', label='Validation')\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.draw()\n",
    "    #plt.subplot(212)\n",
    "    fig2=plt.figure()\n",
    "    plt.title(\"PSNR function on both training set and validation set\")\n",
    "    plt.plot(x,PSNR_epoch_train,'-r',label='PSNR on the training set')\n",
    "    plt.plot(x,PSNR_epoch_val,'-b',label='PSNR on the validation set')\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"PSNR\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.draw()\n",
    "\t\n",
    "\n",
    "if preprocess:\n",
    "  if mode == 'S':\n",
    "    prepare_data(data_path, patch_size=40, stride=10, aug_times=1)\n",
    "  if mode == 'B':\n",
    "    prepare_data(data_path, patch_size=50, stride=10, aug_times=2)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPK4946Eqoz_"
   },
   "source": [
    "# **Testing**\n",
    "The hyperparameters are defined at the beginning of the code:\n",
    "\n",
    "**num_of_layers** = 17 is the number of layers.\n",
    "\n",
    "**logdir**       = '/content/drive/My Drive/Stereo-PyTorch-master/logs/new_training2' is the path to the diractory of the tested model.\n",
    "\n",
    "**test_noiseL**   = 25 is the test noise level.\n",
    "\n",
    "**data_path**   = '/content/drive/My Drive/Stereo-PyTorch-master/data' is the data path.\n",
    "\n",
    "**name_model**    = 'model_050.pth' is the name of the tested model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJiFIw0MlFI9"
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "\"\"\"defining hyperparameters\"\"\"\n",
    "num_of_layers = 17\n",
    "data_path     = '/content/drive/My Drive/Stereo-PyTorch-master/data'\n",
    "logdir        = '/content/drive/My Drive/Stereo-PyTorch-master/logs/new_training2'\n",
    "test_noiseL   = 25\n",
    "name_model    = 'model_050.pth'                          \n",
    "\n",
    "def normalize(data):\n",
    "    return data/255.\n",
    "  \n",
    "def main():\n",
    "    # Build model\n",
    "    print('Loading model ...\\n')\n",
    "    net = DnCNN(channels=2, num_of_layers=17)\n",
    "    device_ids = [0]\n",
    "    model = nn.DataParallel(net, device_ids=device_ids).cuda()\n",
    "    model = torch.load(os.path.join(logdir, name_model))\n",
    "    #model.load_state_dict(torch.load(os.path.join(logdir, 'name_model')))\n",
    "    model.eval()\n",
    "    \n",
    "    # load data info\n",
    "    print('Loading data info ...\\n')\n",
    "    files_left = glob.glob(os.path.join(data_path,'test_left','*.png'))\n",
    "    files_right = glob.glob(os.path.join(data_path,'test_right','*.png'))\n",
    "    files_left.sort()\n",
    "    files_right.sort()\n",
    "    \n",
    "    # process data\n",
    "    psnr_test = 0\n",
    "    ssim_test = 0\n",
    "    for i in range(len(files_left)):\n",
    "        # image\n",
    "        img_right = cv2.imread(files_right[i], 0)\n",
    "        img_left  = cv2.imread(files_left [i], 0)\n",
    "        img_right= np.expand_dims(img_right,2)\n",
    "        img_left = np.expand_dims(img_left,2)\n",
    "        hv, wv, cv= img_right.shape\n",
    "        img=np.zeros((2,hv,wv))\n",
    "        img[0,:,:]=img_right[:,:,0]\n",
    "        img[1,:,:]=img_left[:,:,0]\n",
    "        img = normalize(np.float32(img[:,:,:]))\n",
    "        ISource = torch.Tensor(img)\n",
    "        ISource = torch.unsqueeze(ISource,0)\n",
    "        \n",
    "        # noise\n",
    "        noise = torch.FloatTensor(ISource.size()).normal_(mean=0, std=test_noiseL/255.)\n",
    "        \n",
    "        # noisy image\n",
    "        INoisy = ISource + noise\n",
    "        ISource, INoisy = Variable(ISource.cuda()), Variable(INoisy.cuda())\n",
    "        with torch.no_grad(): # this can save much memory\n",
    "          imgnoisy=model(INoisy)\n",
    "          Out = torch.clamp(INoisy-model(imgnoisy), 0., 1.)\n",
    "          \n",
    "        ## if you are using older version of PyTorch, torch.no_grad() may not be supported\n",
    "        # ISource, INoisy = Variable(ISource.cuda(),volatile=True), Variable(INoisy.cuda(),volatile=True)\n",
    "        # Out = torch.clamp(INoisy-model(INoisy), 0., 1.)\n",
    "        \n",
    "        #PSNR\n",
    "        out_l = Out[:,0,:,:].unsqueeze(0)\n",
    "        out_r = Out[:,1,:,:].unsqueeze(0)\n",
    "        source_l = ISource[:,0,:,:].unsqueeze(0)\n",
    "        source_r = ISource[:,1,:,:].unsqueeze(0)\n",
    "        psnr_right = batch_PSNR(out_l,source_l,None)\n",
    "        psnr_left = batch_PSNR(out_r,source_r,None)\n",
    "        psnr_test += (psnr_left+psnr_right)/2\n",
    "\n",
    "        #SSIM\n",
    "        ssim_right = batch_SSIM(out_l,source_l)\n",
    "        ssim_left = batch_SSIM(out_r,source_r)\n",
    "        ssim_test += (ssim_left+ssim_right)/2\n",
    "        \n",
    "        #Preparing images for dispaly\n",
    "        Out = torch.squeeze(Out)\n",
    "        Out = Out.permute(1,2,0)\n",
    "        image_right_c = Out[:,:,0]\n",
    "        image_left_c = Out[:,:,1]\n",
    "        \n",
    "        \n",
    "        ISource = torch.squeeze(ISource)\n",
    "        ISource = ISource.permute(1,2,0)\n",
    "        image_right_o = ISource[:,:,0]\n",
    "        image_left_o = ISource[:,:,1]\n",
    "        \n",
    "      \n",
    "        INoisy = torch.squeeze(INoisy)\n",
    "        INoisy = INoisy.permute(1,2,0)\n",
    "        image_right_n = INoisy[:,:,0]\n",
    "        image_left_n = INoisy[:,:,1]\n",
    "        \n",
    "        #show images\n",
    "        \"\"\"plt.imshow(image_right_o, cmap=plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "        plt.imshow(image_right_n, cmap=plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "        plt.imshow(image_right_c, cmap=plt.get_cmap('gray'))\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "        print(\"Image Left  \",i+1,\"PSNR: \",psnr_left)\n",
    "        print(\"Image Right \",i+1,\"PSNR: \",psnr_right)\n",
    "    psnr_test /= len(files_right)\n",
    "    ssim_test /= len(files_right)\n",
    "    print(\"\\nPSNR on test data %f \" % psnr_test)\n",
    "    print(\"\\nSSIM on test data %f \" % ssim_test)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Stereo Image Denoising.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
